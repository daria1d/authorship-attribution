{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled40.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouBHxXlbvtdQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy\n",
        "from itertools import islice\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Conv1D, Concatenate, TimeDistributed\n",
        "from tensorflow.keras import utils\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формирование словаря эмбеддингов"
      ],
      "metadata": {
        "id": "ItGxV3aWSp-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_file = open(\"glove.twitter.27B.25d.txt\", 'r')\n",
        "word_dict = {}\n",
        "word_dict['<empty_sent>'] = np.array([0.1] * 25)\n",
        "word_dict['<pad_sent>'] = np.array([0.1] * 25)\n",
        "for line in embedding_file:\n",
        "  word, vector = line.split(maxsplit = 1)\n",
        "  vector = np.fromstring(vector, \"f\", sep = \" \")\n",
        "  word_dict[word] = vector"
      ],
      "metadata": {
        "id": "JwgYy3NS_v_F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка и предобработка данных"
      ],
      "metadata": {
        "id": "QKMf0zgxS5IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = open(\"imdb62.txt\", 'r')\n",
        "data_list = []\n",
        "user_list = []\n",
        "content_list = []\n",
        "for line in dataset:\n",
        "  data_list.append(line)\n",
        "random.shuffle(data_list)\n",
        "for line in data_list:\n",
        "  reviewId, userId, itemId, rating, title, content = line.split('\\t')\n",
        "  user_list.append(userId)\n",
        "  content_list.append(content)"
      ],
      "metadata": {
        "id": "AP2dSFyQ_5WR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(text_data, sent_num, word_num, embed_dim):\n",
        "  vec_data = np.zeros(shape = (len(text_data), sent_num, word_num, embed_dim))\n",
        "  for k in range(len(text_data)):\n",
        "    message = text_data[k]\n",
        "    vec_message = np.zeros(shape = (sent_num, word_num, embed_dim))\n",
        "    sent_list = tokenize.sent_tokenize(message)\n",
        "    sent_list_pad = (sent_list + ['<empty_sent>'] * sent_num)[:sent_num]\n",
        "    for j in range(len(sent_list_pad)):\n",
        "      sentence = sent_list_pad[j]\n",
        "      vec_sentence = np.zeros(shape = (word_num, embed_dim))\n",
        "      word_list = sentence.split()\n",
        "      word_list_pad = (word_list+['<pad_sent>'] * word_num)[:word_num]\n",
        "      for i in range(len(word_list_pad)):\n",
        "        if (lemmatizer.lemmatize(word_list_pad[i])).lower() in word_dict:\n",
        "          vec_sentence[i] = word_dict.get((lemmatizer.lemmatize(word_list_pad[i])).lower())\n",
        "      vec_message[j] = vec_sentence\n",
        "    vec_data[k] = vec_message\n",
        "  return vec_data"
      ],
      "metadata": {
        "id": "F0P3gyCrayqv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 25\n",
        "word_num = 40\n",
        "sent_num = 15"
      ],
      "metadata": {
        "id": "O5RvkUwSMs2N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data=vectorize(text_data = content_list, sent_num = sent_num, word_num = word_num, embed_dim = embed_dim)"
      ],
      "metadata": {
        "id": "QechjJSIAm2x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = text_data[:40000]\n",
        "val_text = text_data[40000:50000]\n",
        "test_text = text_data[50000:62000]"
      ],
      "metadata": {
        "id": "ZPSNrPU4AKnF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_user = set()\n",
        "for user in user_list:\n",
        "  unique_user.add(user)\n",
        "user_index = {}\n",
        "for index, number in enumerate(unique_user):\n",
        "  user_index[number] = index\n",
        "user = []\n",
        "for i in user_list:\n",
        "  user.append(user_index.get(i))"
      ],
      "metadata": {
        "id": "UbE2C6WtCQPS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = utils.to_categorical(user[:40000], 62)\n",
        "val_label = utils.to_categorical(user[40000:50000], 62)\n",
        "test_label = utils.to_categorical(user[50000:62000], 62)"
      ],
      "metadata": {
        "id": "85ESTwj9CQSo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок кодирующего компонента"
      ],
      "metadata": {
        "id": "7uJYdUF1ilcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ffn_dim, rate = 0.1, name = None):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ffn_dim, activation = \"relu\"), \n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_att = LayerNormalization(epsilon = 1e-8)\n",
        "        self.layernorm_ffn = LayerNormalization(epsilon = 1e-8)\n",
        "        self.dropout_att = Dropout(rate)\n",
        "        self.dropout_ffn = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        att_output = self.att(inputs, inputs)\n",
        "        att_output = self.dropout_att(att_output, training = training)\n",
        "        out_att = self.layernorm_att(inputs + att_output)\n",
        "        ffn_output = self.ffn(out_att)\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training = training)\n",
        "        return self.layernorm_ffn(out_att + ffn_output)\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "v0Fd1xL8BSyM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формирование модели"
      ],
      "metadata": {
        "id": "fccSNOIeitlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffn_word_dim = 25\n",
        "ffn_sent_dim = 50\n",
        "word_output_dim = 50\n",
        "sentence_output_dim = 100\n",
        "class_num = 62\n",
        "num_heads = 4\n",
        "dropout_rate = 0.1\n",
        "\n",
        "word_level_input = Input(shape = (word_num, embed_dim), name = 'word_level_input')\n",
        "word_encoder = EncoderBlock(embed_dim = embed_dim, num_heads = num_heads, ffn_dim = ffn_word_dim, name = 'word_encoder')(word_level_input)\n",
        "word_pooling = GlobalAveragePooling1D(name = 'word_pooling')(word_encoder)\n",
        "word_dropout = Dropout(dropout_rate, name = 'word_dropout')(word_pooling)\n",
        "word_level_out = Dense(word_output_dim, activation = 'relu', name = 'word_level_output')(word_dropout)\n",
        "word_encoder = Model(word_level_input, word_level_out)\n",
        "\n",
        "word_encoder.summary()\n",
        "\n",
        "sentence_input = Input(shape = (sent_num, word_num, embed_dim), name = 'sentence_level_input')\n",
        "sentence_repr = TimeDistributed(word_encoder, name = 'sentence_representation')(sentence_input)\n",
        "sentence_encoder = EncoderBlock(embed_dim = word_output_dim, num_heads = num_heads, ffn_dim = ffn_sent_dim, name = 'sentence_encoder')(sentence_repr)\n",
        "sentence_pooling = GlobalAveragePooling1D(name = 'sentence_pooling')(sentence_encoder)\n",
        "sentence_dropout = Dropout(dropout_rate, name = 'sentence_dropout')(sentence_pooling)\n",
        "sentence_level_out = Dense(sentence_output_dim, activation = 'relu', name = 'sentence_level_output')(sentence_dropout)\n",
        "\n",
        "classifier = Dense(class_num , activation = 'softmax', name = 'classifier')(sentence_level_out)\n",
        "\n",
        "model = keras.Model(sentence_input, classifier)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LdqaTvTWlKY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение и оценка модели"
      ],
      "metadata": {
        "id": "vDKd0W24ixyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_text, train_label, \n",
        "                    epochs = 35, batch_size = 32,\n",
        "                    validation_data = (val_text, val_label)\n",
        "                   )"
      ],
      "metadata": {
        "id": "cQZadm1pBj-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_text, test_label, verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "id": "CF2SW2liBvBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
